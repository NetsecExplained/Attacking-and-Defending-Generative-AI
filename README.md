# Attacking-and-Defending-Generative-AI
Reference notes for Attacking and Defending Generative AI presentation

## People to Follow on Twitter
* [Rez0__](https://twitter.com/rez0__)
* [Kai Greshake](https://twitter.com/KGreshake)
* [Johann Rehberger](https://twitter.com/wunderwuzzi23)
* [LLM Security](https://twitter.com/llm_sec)
* [Leon Derczynski](https://twitter.com/LeonDerczynski)
* [Simon Willison](https://twitter.com/simonw)

## LLM Security Frameworks
* [OWASP Top 10 for LLM Applications](https://llmtop10.com/)
* [AVID - AI Vulnerability Database](https://avidml.org/)
* [MITRE - ATLAS](https://atlas.mitre.org/)
* [NIST - Adversarial Machine Learning](https://csrc.nist.gov/pubs/ai/100/2/e2023/final)
  * [NIST AI - TL;DR by Rez0](https://twitter.com/rez0__/status/1743266573668757568?s=20)

## LLM Security Tools
* [Dropbox - LLM security](https://github.com/dropbox/llm-security)
* [Garak vulnerability scanner](https://github.com/leondz/garak)
* [NeMo Guardrails](https://github.com/NVIDIA/NeMo-Guardrails)

## LLM Attack Technique Papers
* [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://llm-attacks.org/)
* [Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild](https://arxiv.org/abs/2311.06237)
* [Understanding Prompt Injection Attacks and Mitigations](https://www.youtube.com/watch?v=MxxPbN9GGYE)
* [Understanding Invisible Prompt Injections](https://laiyer.substack.com/p/securing-against-invisible-prompt?r=2sxk5z)
* [Invisible Prompt Injection Code](https://twitter.com/rez0__/status/1745545813512663203?s=20)
* [GPT4 Technical Paper](https://arxiv.org/pdf/2303.08774.pdf)
* [Netsec Explained - ChatGPT Your Red Team Ally](https://github.com/NetsecExplained/chatgpt-your-red-team-ally)
* [Rez0 - Prompt Injection Primer for Engineers (PIPE)](http://aivillage.org/large%20language%20models/threat-modeling-llm/)
* [LLM Hackers Handbook](https://doublespeak.chat/#/handbook)
* [Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/abs/2302.12173)
* [Kai Greshake - (In)security and Misalignment: A modern security crisis](https://www.youtube.com/watch?v=07rnNHnb9rw)

## LLM Attacks in the Wild
* [Chevy Dealership](https://www.threads.net/@spencerlindsay/post/C08QLzUOFUc/?igshid=NTYzOWQzNmJjMA%3D%3D)
* [Teachers Finding Students Using ChatGPT with Prompt Injections](https://www.instagram.com/reel/C0AetCixWRx/?igshid=MzRlODBiNWFlZA%3D%3D)
* [ChatGPT Unspeakable Tokens](https://www.lesswrong.com/posts/kmWrwtGE9B9hpbgRT/a-search-for-more-chatgpt-gpt-3-5-gpt-4-unspeakable-glitch#_Unspeakable__tokens)
* [Vulnerable AI Paragraph Rewriter - Harmless](https://ahrefs.com/writing-tools/paragraph-rewriter)
* [Insecure Plugin - Chat with Code](https://embracethered.com/blog/posts/2023/chatgpt-chat-with-code-plugin-take-down/)
* [ChatGPT Plugin Exploit Chain - Accessing Private Data](https://embracethered.com/blog/posts/2023/chatgpt-cross-plugin-request-forgery-and-prompt-injection./)

## Mitigation techniques
* [System Prompt to Mitigate Revealing RAG Sources](https://community.openai.com/t/magic-words-can-reveal-all-of-prompts-of-the-gpts/496771/146)
* [AI Village - Threat Modeling LLMs](http://aivillage.org/large%20language%20models/threat-modeling-llm/)
* [NeMo Guardrails - Practical Guide](https://blog.marvik.ai/2023/10/09/enhancing-llama2-conversations-with-nemo-guardrails-a-practical-guide/)
* [OpenAI Moderation API](https://platform.openai.com/docs/guides/moderation/overview)
